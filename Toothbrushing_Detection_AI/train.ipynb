{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1554, 30, 1995)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    'CIRCLE_1', 'CIRCLE_2', 'CIRCLE_3', \n",
    "    'HORIZONTAL_1', 'HORIZONTAL_2', 'HORIZONTAL_3', 'HORIZONTAL_4', \n",
    "    'VERTICAL_1', 'VERTICAL_2'\n",
    "]\n",
    "\n",
    "seq_length = 30 # 30개의 frame으로 구성된 sequence\n",
    "\n",
    "# circle 1\n",
    "\n",
    "circle1_x = np.load('vx_CIRCLE_1.npy') # 학습할 데이터셋 파일 불러오기\n",
    "circle1_y = np.load('vy_CIRCLE_1.npy')\n",
    "circle1_z = np.load('vz_CIRCLE_1.npy')\n",
    "\n",
    "circle1 = np.zeros((len(circle1_x), 4), dtype =np.float32)\n",
    "\n",
    "for i in range(len(circle1_x)):\n",
    "    circle1[i][0] = circle1_x[i]\n",
    "    circle1[i][1] = circle1_y[i]\n",
    "    circle1[i][2] = circle1_z[i]\n",
    "    circle1[i][3] = 0\n",
    "\n",
    "seq_circle1 = []\n",
    "\n",
    "for seq in range(len(circle1) - seq_length+1):\n",
    "    seq_circle1.append(circle1[seq:seq + seq_length])\n",
    "\n",
    "seq_circle1 = np.array(seq_circle1)\n",
    "print(len(circle1_x))\n",
    "print(seq_circle1.shape)\n",
    "print(seq_circle1)\n",
    "\n",
    "\n",
    "# circle 2\n",
    "\n",
    "circle2_x = np.load('vx_CIRCLE_2.npy')\n",
    "circle2_y = np.load('vy_CIRCLE_2.npy')\n",
    "circle2_z = np.load('vz_CIRCLE_2.npy')\n",
    "\n",
    "circle2 = np.zeros((len(circle2_x), 4), dtype =np.float32)\n",
    "\n",
    "for i in range(len(circle2_x)):\n",
    "    circle2[i][0] = circle2_x[i]\n",
    "    circle2[i][1] = circle2_y[i]\n",
    "    circle2[i][2] = circle2_z[i]\n",
    "    circle2[i][3] = 1\n",
    "\n",
    "seq_circle2 = []\n",
    "\n",
    "for seq in range(len(circle2) - seq_length):\n",
    "    seq_circle2.append(circle2[seq:seq + seq_length])\n",
    "\n",
    "seq_circle2 = np.array(seq_circle2)\n",
    "\n",
    "# circle 3\n",
    "\n",
    "circle3_x = np.load('vx_CIRCLE_3.npy')\n",
    "circle3_y = np.load('vy_CIRCLE_3.npy')\n",
    "circle3_z = np.load('vz_CIRCLE_3.npy')\n",
    "\n",
    "circle3 = np.zeros((len(circle3_x), 4), dtype =np.float32)\n",
    "\n",
    "for i in range(len(circle3_x)):\n",
    "    circle3[i][0] = circle3_x[i]\n",
    "    circle3[i][1] = circle3_y[i]\n",
    "    circle3[i][2] = circle3_z[i]\n",
    "    circle3[i][3] = 2\n",
    "\n",
    "seq_circle3 = []\n",
    "\n",
    "for seq in range(len(circle3) - seq_length):\n",
    "    seq_circle3.append(circle3[seq:seq + seq_length])\n",
    "\n",
    "seq_circle3 = np.array(seq_circle3)\n",
    "\n",
    "# horizontal 1\n",
    "\n",
    "horizontal1_x = np.load('vx_HORIZONTAL_1.npy')\n",
    "horizontal1_y = np.load('vy_HORIZONTAL_1.npy')\n",
    "horizontal1_z = np.load('vz_HORIZONTAL_1.npy')\n",
    "\n",
    "horizontal1 = np.zeros((len(horizontal1_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(horizontal1_x)):\n",
    "    horizontal1[i][0] = horizontal1_x[i]\n",
    "    horizontal1[i][1] = horizontal1_y[i]\n",
    "    horizontal1[i][2] = horizontal1_z[i]\n",
    "    horizontal1[i][3] = 3\n",
    "\n",
    "seq_horizontal1 = []\n",
    "\n",
    "for seq in range(len(horizontal1) - seq_length):\n",
    "    seq_horizontal1.append(horizontal1[seq:seq + seq_length])\n",
    "\n",
    "seq_horizontal1 = np.array(seq_horizontal1)\n",
    "\n",
    "# horizontal 2\n",
    "\n",
    "horizontal2_x = np.load('vx_HORIZONTAL_2.npy')\n",
    "horizontal2_y = np.load('vy_HORIZONTAL_2.npy')\n",
    "horizontal2_z = np.load('vz_HORIZONTAL_2.npy')\n",
    "\n",
    "horizontal2 = np.zeros((len(horizontal2_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(horizontal2_x)):\n",
    "    horizontal2[i][0] = horizontal2_x[i]\n",
    "    horizontal2[i][1] = horizontal2_y[i]\n",
    "    horizontal2[i][2] = horizontal2_z[i]\n",
    "    horizontal2[i][3] = 4\n",
    "\n",
    "seq_horizontal2 = []\n",
    "\n",
    "for seq in range(len(horizontal2) - seq_length):\n",
    "    seq_horizontal2.append(horizontal2[seq:seq + seq_length])\n",
    "\n",
    "seq_horizontal2 = np.array(seq_horizontal2)\n",
    "\n",
    "# horizontal 3\n",
    "\n",
    "horizontal3_x = np.load('vx_HORIZONTAL_3.npy')\n",
    "horizontal3_y = np.load('vy_HORIZONTAL_3.npy')\n",
    "horizontal3_z = np.load('vz_HORIZONTAL_3.npy')\n",
    "\n",
    "horizontal3 = np.zeros((len(horizontal3_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(horizontal3_x)):\n",
    "    horizontal3[i][0] = horizontal3_x[i]\n",
    "    horizontal3[i][1] = horizontal3_y[i]\n",
    "    horizontal3[i][2] = horizontal3_z[i]\n",
    "    horizontal3[i][3] = 5\n",
    "\n",
    "seq_horizontal3 = []\n",
    "\n",
    "for seq in range(len(horizontal3) - seq_length):\n",
    "    seq_horizontal3.append(horizontal3[seq:seq + seq_length])\n",
    "\n",
    "seq_horizontal3 = np.array(seq_horizontal3)\n",
    "\n",
    "# horizontal 4\n",
    "\n",
    "horizontal4_x = np.load('vx_HORIZONTAL_4.npy')\n",
    "horizontal4_y = np.load('vy_HORIZONTAL_4.npy')\n",
    "horizontal4_z = np.load('vz_HORIZONTAL_4.npy')\n",
    "\n",
    "horizontal4 = np.zeros((len(horizontal4_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(horizontal4_x)):\n",
    "    horizontal4[i][0] = horizontal4_x[i]\n",
    "    horizontal4[i][1] = horizontal4_y[i]\n",
    "    horizontal4[i][2] = horizontal4_z[i]\n",
    "    horizontal4[i][3] = 6\n",
    "\n",
    "seq_horizontal4 = []\n",
    "\n",
    "for seq in range(len(horizontal4) - seq_length):\n",
    "    seq_horizontal4.append(horizontal4[seq:seq + seq_length])\n",
    "\n",
    "seq_horizontal4 = np.array(seq_horizontal4)\n",
    "\n",
    "\n",
    "# vertical 1\n",
    "\n",
    "vertical1_x = np.load('vx_VERTICAL_1.npy')\n",
    "vertical1_y = np.load('vy_VERTICAL_1.npy')\n",
    "vertical1_z = np.load('vz_VERTICAL_1.npy')\n",
    "\n",
    "vertical1 = np.zeros((len(vertical1_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(vertical1_x)):\n",
    "    vertical1[i][0] = vertical1_x[i]\n",
    "    vertical1[i][1] = vertical1_y[i]\n",
    "    vertical1[i][2] = vertical1_z[i]\n",
    "    vertical1[i][3] = 7\n",
    "\n",
    "seq_vertical1 = []\n",
    "\n",
    "for seq in range(len(vertical1) - seq_length):\n",
    "    seq_vertical1.append(vertical1[seq:seq + seq_length])\n",
    "\n",
    "seq_vertical1 = np.array(seq_vertical1)\n",
    "\n",
    "\n",
    "# vertical 2\n",
    "\n",
    "vertical2_x = np.load('vx_VERTICAL_2.npy')\n",
    "vertical2_y = np.load('vy_VERTICAL_2.npy')\n",
    "vertical2_z = np.load('vz_VERTICAL_2.npy')\n",
    "\n",
    "vertical2 = np.zeros((len(vertical2_x), 4), dtype = np.float32)\n",
    "\n",
    "for i in range(len(vertical2_x)):\n",
    "    vertical2[i][0] = vertical2_x[i]\n",
    "    vertical2[i][1] = vertical2_y[i]\n",
    "    vertical2[i][2] = vertical2_z[i]\n",
    "    vertical2[i][3] = 8\n",
    "\n",
    "seq_vertical2 = []\n",
    "\n",
    "for seq in range(len(vertical2) - seq_length):\n",
    "    seq_vertical2.append(vertical2[seq:seq + seq_length])\n",
    "\n",
    "seq_vertical2 = np.array(seq_vertical2)\n",
    "\n",
    "print()\n",
    "\n",
    "data = np.concatenate([ # 학습한 데이터셋 파일 불러오기\n",
    "    seq_circle1, seq_circle2, seq_circle3,\n",
    "    seq_horizontal1, seq_horizontal2, seq_horizontal3, seq_horizontal4,\n",
    "    seq_vertical1, seq_vertical2\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1554, 30, 1994)\n",
      "(1554,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape) # label - one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1554, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1398, 30, 1994) (1398, 3)\n",
      "(156, 30, 1994) (156, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                527104    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 529283 (2.02 MB)\n",
      "Trainable params: 529283 (2.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# sequential model\n",
    "# LSTM + Dense\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 11.3779 - acc: 0.5172\n",
      "Epoch 1: val_acc improved from -inf to 0.39744, saving model to models\\model.h5\n",
      "44/44 [==============================] - 3s 56ms/step - loss: 11.3779 - acc: 0.5172 - val_loss: 19.8197 - val_acc: 0.3974 - lr: 0.0010\n",
      "Epoch 2/200\n",
      " 3/44 [=>............................] - ETA: 1s - loss: 17.7262 - acc: 0.4583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y10cuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 9.5099 - acc: 0.4738\n",
      "Epoch 2: val_acc improved from 0.39744 to 0.64103, saving model to models\\model.h5\n",
      "44/44 [==============================] - 2s 52ms/step - loss: 9.3702 - acc: 0.4778 - val_loss: 1.0151 - val_acc: 0.6410 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.7225 - acc: 0.7042\n",
      "Epoch 3: val_acc improved from 0.64103 to 0.98718, saving model to models\\model.h5\n",
      "44/44 [==============================] - 2s 49ms/step - loss: 0.7139 - acc: 0.7089 - val_loss: 0.1322 - val_acc: 0.9872 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 4: val_acc improved from 0.98718 to 1.00000, saving model to models\\model.h5\n",
      "44/44 [==============================] - 2s 51ms/step - loss: 0.0372 - acc: 1.0000 - val_loss: 0.0199 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 2s 52ms/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 2s 53ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0096 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 58ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 9.7150e-04 - acc: 1.0000\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 9.7006e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 8.3007e-04 - acc: 1.0000\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 8.2789e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 7.1371e-04 - acc: 1.0000\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 62ms/step - loss: 7.1158e-04 - acc: 1.0000 - val_loss: 9.4726e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 5.9888e-04 - acc: 1.0000\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 61ms/step - loss: 6.1146e-04 - acc: 1.0000 - val_loss: 8.6627e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 6.6888e-04 - acc: 1.0000\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 62ms/step - loss: 6.7134e-04 - acc: 1.0000 - val_loss: 8.6094e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 5.8519e-04 - acc: 1.0000\n",
      "Epoch 21: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 61ms/step - loss: 5.8285e-04 - acc: 1.0000 - val_loss: 7.8231e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.7480e-04 - acc: 1.0000\n",
      "Epoch 22: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 61ms/step - loss: 4.7397e-04 - acc: 1.0000 - val_loss: 6.9146e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.2022e-04 - acc: 1.0000\n",
      "Epoch 23: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 63ms/step - loss: 4.1947e-04 - acc: 1.0000 - val_loss: 6.2345e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.7705e-04 - acc: 1.0000\n",
      "Epoch 24: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 64ms/step - loss: 3.7699e-04 - acc: 1.0000 - val_loss: 5.7378e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.4363e-04 - acc: 1.0000\n",
      "Epoch 25: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 65ms/step - loss: 3.4173e-04 - acc: 1.0000 - val_loss: 5.1528e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.1290e-04 - acc: 1.0000\n",
      "Epoch 26: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 65ms/step - loss: 3.1214e-04 - acc: 1.0000 - val_loss: 4.6215e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.8580e-04 - acc: 1.0000\n",
      "Epoch 27: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 63ms/step - loss: 2.8664e-04 - acc: 1.0000 - val_loss: 4.1741e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.6506e-04 - acc: 1.0000\n",
      "Epoch 28: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 63ms/step - loss: 2.6424e-04 - acc: 1.0000 - val_loss: 3.8032e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.4507e-04 - acc: 1.0000\n",
      "Epoch 29: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 2.4507e-04 - acc: 1.0000 - val_loss: 3.4949e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.2823e-04 - acc: 1.0000\n",
      "Epoch 30: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 2.2823e-04 - acc: 1.0000 - val_loss: 3.2338e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.1346e-04 - acc: 1.0000\n",
      "Epoch 31: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 2.1346e-04 - acc: 1.0000 - val_loss: 2.9260e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0016e-04 - acc: 1.0000\n",
      "Epoch 32: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 2.0016e-04 - acc: 1.0000 - val_loss: 2.6813e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8815e-04 - acc: 1.0000\n",
      "Epoch 33: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 1.8815e-04 - acc: 1.0000 - val_loss: 2.4968e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7746e-04 - acc: 1.0000\n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 1.7746e-04 - acc: 1.0000 - val_loss: 2.2968e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6761e-04 - acc: 1.0000\n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 1.6761e-04 - acc: 1.0000 - val_loss: 2.1524e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5861e-04 - acc: 1.0000\n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 1.5861e-04 - acc: 1.0000 - val_loss: 1.9900e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5032e-04 - acc: 1.0000\n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 1.5032e-04 - acc: 1.0000 - val_loss: 1.8701e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4267e-04 - acc: 1.0000\n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 1.4267e-04 - acc: 1.0000 - val_loss: 1.7359e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3552e-04 - acc: 1.0000\n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 1.3552e-04 - acc: 1.0000 - val_loss: 1.6557e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2886e-04 - acc: 1.0000\n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 1.2886e-04 - acc: 1.0000 - val_loss: 1.5509e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2270e-04 - acc: 1.0000\n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 80ms/step - loss: 1.2270e-04 - acc: 1.0000 - val_loss: 1.4495e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1682e-04 - acc: 1.0000\n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 1.1682e-04 - acc: 1.0000 - val_loss: 1.3748e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1135e-04 - acc: 1.0000\n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 1.1135e-04 - acc: 1.0000 - val_loss: 1.2957e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0618e-04 - acc: 1.0000\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 1.0618e-04 - acc: 1.0000 - val_loss: 1.2291e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0126e-04 - acc: 1.0000\n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.0126e-04 - acc: 1.0000 - val_loss: 1.1679e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.6619e-05 - acc: 1.0000\n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 9.6619e-05 - acc: 1.0000 - val_loss: 1.1146e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.2204e-05 - acc: 1.0000\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 9.2204e-05 - acc: 1.0000 - val_loss: 1.0514e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.7973e-05 - acc: 1.0000\n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 8.7973e-05 - acc: 1.0000 - val_loss: 9.9937e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.3996e-05 - acc: 1.0000\n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 8.3996e-05 - acc: 1.0000 - val_loss: 9.4613e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.0112e-05 - acc: 1.0000\n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 8.0112e-05 - acc: 1.0000 - val_loss: 9.0066e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.6466e-05 - acc: 1.0000\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 80ms/step - loss: 7.6466e-05 - acc: 1.0000 - val_loss: 8.5771e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.2900e-05 - acc: 1.0000\n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 7.2900e-05 - acc: 1.0000 - val_loss: 8.1383e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.9499e-05 - acc: 1.0000\n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 6.9499e-05 - acc: 1.0000 - val_loss: 7.7191e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.6173e-05 - acc: 1.0000\n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 6.6173e-05 - acc: 1.0000 - val_loss: 7.3728e-05 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.3740e-05 - acc: 1.0000\n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 6.3740e-05 - acc: 1.0000 - val_loss: 7.1776e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.2147e-05 - acc: 1.0000\n",
      "Epoch 56: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 6.2147e-05 - acc: 1.0000 - val_loss: 6.9993e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.0559e-05 - acc: 1.0000\n",
      "Epoch 57: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 6.0559e-05 - acc: 1.0000 - val_loss: 6.8162e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.8989e-05 - acc: 1.0000\n",
      "Epoch 58: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 5.8989e-05 - acc: 1.0000 - val_loss: 6.6200e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.7400e-05 - acc: 1.0000\n",
      "Epoch 59: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 5.7400e-05 - acc: 1.0000 - val_loss: 6.4375e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.5839e-05 - acc: 1.0000\n",
      "Epoch 60: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 5.5839e-05 - acc: 1.0000 - val_loss: 6.2591e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 5.4415e-05 - acc: 1.0000\n",
      "Epoch 61: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 65ms/step - loss: 5.4282e-05 - acc: 1.0000 - val_loss: 6.0822e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.2774e-05 - acc: 1.0000\n",
      "Epoch 62: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 5.2774e-05 - acc: 1.0000 - val_loss: 5.9033e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.1203e-05 - acc: 1.0000\n",
      "Epoch 63: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 5.1203e-05 - acc: 1.0000 - val_loss: 5.7191e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.9701e-05 - acc: 1.0000\n",
      "Epoch 64: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 4.9701e-05 - acc: 1.0000 - val_loss: 5.5376e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.8227e-05 - acc: 1.0000\n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 4.8166e-05 - acc: 1.0000 - val_loss: 5.3699e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.6799e-05 - acc: 1.0000\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 4.6691e-05 - acc: 1.0000 - val_loss: 5.2018e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.5137e-05 - acc: 1.0000\n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 4.5265e-05 - acc: 1.0000 - val_loss: 5.0201e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.2526e-05 - acc: 1.0000\n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 4.3792e-05 - acc: 1.0000 - val_loss: 4.8424e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.2362e-05 - acc: 1.0000\n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 4.2362e-05 - acc: 1.0000 - val_loss: 4.6658e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.0965e-05 - acc: 1.0000\n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 4.0965e-05 - acc: 1.0000 - val_loss: 4.4922e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 71/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.9743e-05 - acc: 1.0000\n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 3.9548e-05 - acc: 1.0000 - val_loss: 4.3182e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 72/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.8464e-05 - acc: 1.0000\n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 3.8173e-05 - acc: 1.0000 - val_loss: 4.1572e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 73/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 3.6966e-05 - acc: 1.0000\n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 3.6875e-05 - acc: 1.0000 - val_loss: 4.0028e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 74/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.5638e-05 - acc: 1.0000\n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 3.5638e-05 - acc: 1.0000 - val_loss: 3.8534e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 75/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.4444e-05 - acc: 1.0000\n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 3.4444e-05 - acc: 1.0000 - val_loss: 3.7135e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.3310e-05 - acc: 1.0000\n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 3.3310e-05 - acc: 1.0000 - val_loss: 3.5820e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.2229e-05 - acc: 1.0000\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 3.2229e-05 - acc: 1.0000 - val_loss: 3.4595e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.1174e-05 - acc: 1.0000\n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 3.1174e-05 - acc: 1.0000 - val_loss: 3.3393e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.0175e-05 - acc: 1.0000\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 3.0175e-05 - acc: 1.0000 - val_loss: 3.2280e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.9216e-05 - acc: 1.0000\n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 2.9216e-05 - acc: 1.0000 - val_loss: 3.1169e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.8274e-05 - acc: 1.0000\n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 2.8274e-05 - acc: 1.0000 - val_loss: 3.0131e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.7398e-05 - acc: 1.0000\n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 2.7398e-05 - acc: 1.0000 - val_loss: 2.9156e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.6621e-05 - acc: 1.0000\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 2.6534e-05 - acc: 1.0000 - val_loss: 2.8204e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.5703e-05 - acc: 1.0000\n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 2.5703e-05 - acc: 1.0000 - val_loss: 2.7375e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.4903e-05 - acc: 1.0000\n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 2.4903e-05 - acc: 1.0000 - val_loss: 2.6481e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.4155e-05 - acc: 1.0000\n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 2.4155e-05 - acc: 1.0000 - val_loss: 2.5670e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.3401e-05 - acc: 1.0000\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 2.3401e-05 - acc: 1.0000 - val_loss: 2.4875e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.2688e-05 - acc: 1.0000\n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 2.2688e-05 - acc: 1.0000 - val_loss: 2.4164e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.2001e-05 - acc: 1.0000\n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 2.2001e-05 - acc: 1.0000 - val_loss: 2.3415e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 90/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.1344e-05 - acc: 1.0000\n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 2.1344e-05 - acc: 1.0000 - val_loss: 2.2753e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 91/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0706e-05 - acc: 1.0000\n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 2.0706e-05 - acc: 1.0000 - val_loss: 2.2113e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 92/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0098e-05 - acc: 1.0000\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 2.0098e-05 - acc: 1.0000 - val_loss: 2.1509e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 93/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.9491e-05 - acc: 1.0000\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.9491e-05 - acc: 1.0000 - val_loss: 2.0847e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 94/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8917e-05 - acc: 1.0000\n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.8917e-05 - acc: 1.0000 - val_loss: 2.0257e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 95/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8358e-05 - acc: 1.0000\n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.8358e-05 - acc: 1.0000 - val_loss: 1.9721e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 96/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7817e-05 - acc: 1.0000\n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 1.7817e-05 - acc: 1.0000 - val_loss: 1.9193e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 97/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.7299e-05 - acc: 1.0000\n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 95ms/step - loss: 1.7299e-05 - acc: 1.0000 - val_loss: 1.8676e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 98/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6790e-05 - acc: 1.0000\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.6790e-05 - acc: 1.0000 - val_loss: 1.8166e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6306e-05 - acc: 1.0000\n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 1.6306e-05 - acc: 1.0000 - val_loss: 1.7677e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5839e-05 - acc: 1.0000\n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.5839e-05 - acc: 1.0000 - val_loss: 1.7165e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.5381e-05 - acc: 1.0000\n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 1.5381e-05 - acc: 1.0000 - val_loss: 1.6684e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4939e-05 - acc: 1.0000\n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 5s 106ms/step - loss: 1.4939e-05 - acc: 1.0000 - val_loss: 1.6220e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 103/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4504e-05 - acc: 1.0000\n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.4504e-05 - acc: 1.0000 - val_loss: 1.5763e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 104/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.4092e-05 - acc: 1.0000\n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 1.4092e-05 - acc: 1.0000 - val_loss: 1.5335e-05 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 105/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3788e-05 - acc: 1.0000\n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.3788e-05 - acc: 1.0000 - val_loss: 1.5131e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 106/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3585e-05 - acc: 1.0000\n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 84ms/step - loss: 1.3585e-05 - acc: 1.0000 - val_loss: 1.4909e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 107/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3386e-05 - acc: 1.0000\n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 1.3386e-05 - acc: 1.0000 - val_loss: 1.4701e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 108/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.3189e-05 - acc: 1.0000\n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 1.3189e-05 - acc: 1.0000 - val_loss: 1.4492e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 109/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2995e-05 - acc: 1.0000\n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 1.2995e-05 - acc: 1.0000 - val_loss: 1.4284e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 110/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2800e-05 - acc: 1.0000\n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 1.2800e-05 - acc: 1.0000 - val_loss: 1.4066e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 111/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2605e-05 - acc: 1.0000\n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 1.2605e-05 - acc: 1.0000 - val_loss: 1.3861e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 112/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2411e-05 - acc: 1.0000\n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 1.2411e-05 - acc: 1.0000 - val_loss: 1.3665e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 113/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2225e-05 - acc: 1.0000\n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 1.2225e-05 - acc: 1.0000 - val_loss: 1.3466e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 114/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.2038e-05 - acc: 1.0000\n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 1.2038e-05 - acc: 1.0000 - val_loss: 1.3248e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 115/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1844e-05 - acc: 1.0000\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 1.1844e-05 - acc: 1.0000 - val_loss: 1.3056e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 116/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1725e-05 - acc: 1.0000\n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 1.1659e-05 - acc: 1.0000 - val_loss: 1.2851e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 117/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1440e-05 - acc: 1.0000\n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 1.1473e-05 - acc: 1.0000 - val_loss: 1.2655e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 118/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.1289e-05 - acc: 1.0000\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 1.1289e-05 - acc: 1.0000 - val_loss: 1.2447e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 119/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1100e-05 - acc: 1.0000\n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 1.1105e-05 - acc: 1.0000 - val_loss: 1.2258e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 120/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0872e-05 - acc: 1.0000\n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 1.0929e-05 - acc: 1.0000 - val_loss: 1.2065e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 121/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0761e-05 - acc: 1.0000\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 1.0749e-05 - acc: 1.0000 - val_loss: 1.1864e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 122/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0588e-05 - acc: 1.0000\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 1.0578e-05 - acc: 1.0000 - val_loss: 1.1680e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 123/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0404e-05 - acc: 1.0000\n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 1.0404e-05 - acc: 1.0000 - val_loss: 1.1485e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 124/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0322e-05 - acc: 1.0000\n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 1.0226e-05 - acc: 1.0000 - val_loss: 1.1286e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 125/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.0056e-05 - acc: 1.0000\n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 1.0056e-05 - acc: 1.0000 - val_loss: 1.1107e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 126/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.8832e-06 - acc: 1.0000\n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 9.8832e-06 - acc: 1.0000 - val_loss: 1.0910e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 127/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.7170e-06 - acc: 1.0000\n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 9.7170e-06 - acc: 1.0000 - val_loss: 1.0736e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 128/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.5516e-06 - acc: 1.0000\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 9.5516e-06 - acc: 1.0000 - val_loss: 1.0555e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 129/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.3840e-06 - acc: 1.0000\n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 9.3840e-06 - acc: 1.0000 - val_loss: 1.0379e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 130/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.2239e-06 - acc: 1.0000\n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 9.2239e-06 - acc: 1.0000 - val_loss: 1.0192e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 131/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 9.0612e-06 - acc: 1.0000\n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 9.0612e-06 - acc: 1.0000 - val_loss: 1.0017e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 132/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.9013e-06 - acc: 1.0000\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 8.9013e-06 - acc: 1.0000 - val_loss: 9.8384e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 133/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.7470e-06 - acc: 1.0000\n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 8.7470e-06 - acc: 1.0000 - val_loss: 9.6749e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 134/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.5891e-06 - acc: 1.0000\n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 8.5891e-06 - acc: 1.0000 - val_loss: 9.4938e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 135/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.4333e-06 - acc: 1.0000\n",
      "Epoch 135: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 8.4333e-06 - acc: 1.0000 - val_loss: 9.3257e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 136/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.2854e-06 - acc: 1.0000\n",
      "Epoch 136: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 8.2854e-06 - acc: 1.0000 - val_loss: 9.1568e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 137/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 8.1361e-06 - acc: 1.0000\n",
      "Epoch 137: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 76ms/step - loss: 8.1361e-06 - acc: 1.0000 - val_loss: 8.9879e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 138/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.9880e-06 - acc: 1.0000\n",
      "Epoch 138: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 7.9880e-06 - acc: 1.0000 - val_loss: 8.8267e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 139/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.8421e-06 - acc: 1.0000\n",
      "Epoch 139: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 7.8421e-06 - acc: 1.0000 - val_loss: 8.6685e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 140/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.7015e-06 - acc: 1.0000\n",
      "Epoch 140: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 7.7015e-06 - acc: 1.0000 - val_loss: 8.5080e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 141/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.5595e-06 - acc: 1.0000\n",
      "Epoch 141: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 7.5595e-06 - acc: 1.0000 - val_loss: 8.3514e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 142/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.4213e-06 - acc: 1.0000\n",
      "Epoch 142: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 79ms/step - loss: 7.4213e-06 - acc: 1.0000 - val_loss: 8.1963e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 143/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.2839e-06 - acc: 1.0000\n",
      "Epoch 143: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 81ms/step - loss: 7.2839e-06 - acc: 1.0000 - val_loss: 8.0480e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 144/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.1482e-06 - acc: 1.0000\n",
      "Epoch 144: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 7.1482e-06 - acc: 1.0000 - val_loss: 7.8937e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 145/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 7.0146e-06 - acc: 1.0000\n",
      "Epoch 145: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 84ms/step - loss: 7.0146e-06 - acc: 1.0000 - val_loss: 7.7531e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 146/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.8878e-06 - acc: 1.0000\n",
      "Epoch 146: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 6.8878e-06 - acc: 1.0000 - val_loss: 7.6102e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 147/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.7567e-06 - acc: 1.0000\n",
      "Epoch 147: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 6.7567e-06 - acc: 1.0000 - val_loss: 7.4680e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 148/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.6308e-06 - acc: 1.0000\n",
      "Epoch 148: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 6.6308e-06 - acc: 1.0000 - val_loss: 7.3290e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.5059e-06 - acc: 1.0000\n",
      "Epoch 149: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 6.5059e-06 - acc: 1.0000 - val_loss: 7.1929e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.3865e-06 - acc: 1.0000\n",
      "Epoch 150: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 91ms/step - loss: 6.3865e-06 - acc: 1.0000 - val_loss: 7.0554e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.2651e-06 - acc: 1.0000\n",
      "Epoch 151: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 6.2651e-06 - acc: 1.0000 - val_loss: 6.9232e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.1459e-06 - acc: 1.0000\n",
      "Epoch 152: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 6.1459e-06 - acc: 1.0000 - val_loss: 6.7895e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 153/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 6.0309e-06 - acc: 1.0000\n",
      "Epoch 153: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 102ms/step - loss: 6.0309e-06 - acc: 1.0000 - val_loss: 6.6657e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 154/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.9177e-06 - acc: 1.0000\n",
      "Epoch 154: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "44/44 [==============================] - 5s 104ms/step - loss: 5.9177e-06 - acc: 1.0000 - val_loss: 6.5419e-06 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 155/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.8293e-06 - acc: 1.0000\n",
      "Epoch 155: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 5s 105ms/step - loss: 5.8293e-06 - acc: 1.0000 - val_loss: 6.4762e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 156/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.7758e-06 - acc: 1.0000\n",
      "Epoch 156: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 5s 106ms/step - loss: 5.7758e-06 - acc: 1.0000 - val_loss: 6.4158e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 157/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.7203e-06 - acc: 1.0000\n",
      "Epoch 157: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 5s 104ms/step - loss: 5.7203e-06 - acc: 1.0000 - val_loss: 6.3478e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 158/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.6630e-06 - acc: 1.0000\n",
      "Epoch 158: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 5.6630e-06 - acc: 1.0000 - val_loss: 6.2897e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 159/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.6088e-06 - acc: 1.0000\n",
      "Epoch 159: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 91ms/step - loss: 5.6088e-06 - acc: 1.0000 - val_loss: 6.2263e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 160/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.5544e-06 - acc: 1.0000\n",
      "Epoch 160: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 5.5544e-06 - acc: 1.0000 - val_loss: 6.1644e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 161/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.4962e-06 - acc: 1.0000\n",
      "Epoch 161: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 85ms/step - loss: 5.4962e-06 - acc: 1.0000 - val_loss: 6.1002e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 162/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.4424e-06 - acc: 1.0000\n",
      "Epoch 162: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 5.4424e-06 - acc: 1.0000 - val_loss: 6.0353e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 163/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.3867e-06 - acc: 1.0000\n",
      "Epoch 163: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 80ms/step - loss: 5.3867e-06 - acc: 1.0000 - val_loss: 5.9734e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 164/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.3314e-06 - acc: 1.0000\n",
      "Epoch 164: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 5.3314e-06 - acc: 1.0000 - val_loss: 5.9122e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 165/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.2735e-06 - acc: 1.0000\n",
      "Epoch 165: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 5.2735e-06 - acc: 1.0000 - val_loss: 5.8419e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 166/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.2191e-06 - acc: 1.0000\n",
      "Epoch 166: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 5.2191e-06 - acc: 1.0000 - val_loss: 5.7762e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 167/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.1634e-06 - acc: 1.0000\n",
      "Epoch 167: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 74ms/step - loss: 5.1634e-06 - acc: 1.0000 - val_loss: 5.7235e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 168/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.1081e-06 - acc: 1.0000\n",
      "Epoch 168: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 5.1081e-06 - acc: 1.0000 - val_loss: 5.6532e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 169/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 5.0509e-06 - acc: 1.0000\n",
      "Epoch 169: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 5.0509e-06 - acc: 1.0000 - val_loss: 5.5890e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 170/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.9943e-06 - acc: 1.0000\n",
      "Epoch 170: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 4.9943e-06 - acc: 1.0000 - val_loss: 5.5271e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 171/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.9407e-06 - acc: 1.0000\n",
      "Epoch 171: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 4.9407e-06 - acc: 1.0000 - val_loss: 5.4660e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 172/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.7047e-06 - acc: 1.0000\n",
      "Epoch 172: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 4.8842e-06 - acc: 1.0000 - val_loss: 5.4033e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 173/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.8283e-06 - acc: 1.0000\n",
      "Epoch 173: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 4.8283e-06 - acc: 1.0000 - val_loss: 5.3437e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 174/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.7726e-06 - acc: 1.0000\n",
      "Epoch 174: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 4.7739e-06 - acc: 1.0000 - val_loss: 5.2765e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 175/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.7188e-06 - acc: 1.0000\n",
      "Epoch 175: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 4.7188e-06 - acc: 1.0000 - val_loss: 5.2115e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 176/200\n",
      "43/44 [============================>.] - ETA: 0s - loss: 4.6849e-06 - acc: 1.0000\n",
      "Epoch 176: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 4.6626e-06 - acc: 1.0000 - val_loss: 5.1557e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 177/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.6079e-06 - acc: 1.0000\n",
      "Epoch 177: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 4.6079e-06 - acc: 1.0000 - val_loss: 5.0908e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 178/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.5533e-06 - acc: 1.0000\n",
      "Epoch 178: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 4.5533e-06 - acc: 1.0000 - val_loss: 5.0297e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 179/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.4979e-06 - acc: 1.0000\n",
      "Epoch 179: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 4.4979e-06 - acc: 1.0000 - val_loss: 4.9639e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 180/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.4432e-06 - acc: 1.0000\n",
      "Epoch 180: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 4.4432e-06 - acc: 1.0000 - val_loss: 4.9066e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 181/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.3877e-06 - acc: 1.0000\n",
      "Epoch 181: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 4.3877e-06 - acc: 1.0000 - val_loss: 4.8386e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 182/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.3345e-06 - acc: 1.0000\n",
      "Epoch 182: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 4.3345e-06 - acc: 1.0000 - val_loss: 4.7874e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 183/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.2817e-06 - acc: 1.0000\n",
      "Epoch 183: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 4.2817e-06 - acc: 1.0000 - val_loss: 4.7255e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 184/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.2263e-06 - acc: 1.0000\n",
      "Epoch 184: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 4.2263e-06 - acc: 1.0000 - val_loss: 4.6613e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 185/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.1745e-06 - acc: 1.0000\n",
      "Epoch 185: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 4.1745e-06 - acc: 1.0000 - val_loss: 4.5994e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 186/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.1195e-06 - acc: 1.0000\n",
      "Epoch 186: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 4.1195e-06 - acc: 1.0000 - val_loss: 4.5391e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 187/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.0654e-06 - acc: 1.0000\n",
      "Epoch 187: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 4.0654e-06 - acc: 1.0000 - val_loss: 4.4810e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 188/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 4.0145e-06 - acc: 1.0000\n",
      "Epoch 188: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 4.0145e-06 - acc: 1.0000 - val_loss: 4.4191e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 189/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.9617e-06 - acc: 1.0000\n",
      "Epoch 189: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 3.9617e-06 - acc: 1.0000 - val_loss: 4.3603e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 190/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.9090e-06 - acc: 1.0000\n",
      "Epoch 190: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 77ms/step - loss: 3.9090e-06 - acc: 1.0000 - val_loss: 4.3045e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 191/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.8579e-06 - acc: 1.0000\n",
      "Epoch 191: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 75ms/step - loss: 3.8579e-06 - acc: 1.0000 - val_loss: 4.2449e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 192/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.8042e-06 - acc: 1.0000\n",
      "Epoch 192: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 3.8042e-06 - acc: 1.0000 - val_loss: 4.1922e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 193/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.7534e-06 - acc: 1.0000\n",
      "Epoch 193: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 79ms/step - loss: 3.7534e-06 - acc: 1.0000 - val_loss: 4.1287e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 194/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.7017e-06 - acc: 1.0000\n",
      "Epoch 194: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 3.7017e-06 - acc: 1.0000 - val_loss: 4.0737e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 195/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.6508e-06 - acc: 1.0000\n",
      "Epoch 195: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 80ms/step - loss: 3.6508e-06 - acc: 1.0000 - val_loss: 4.0195e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 196/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.6010e-06 - acc: 1.0000\n",
      "Epoch 196: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 3s 78ms/step - loss: 3.6010e-06 - acc: 1.0000 - val_loss: 3.9560e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 197/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.5519e-06 - acc: 1.0000\n",
      "Epoch 197: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 80ms/step - loss: 3.5519e-06 - acc: 1.0000 - val_loss: 3.8987e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 198/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.5019e-06 - acc: 1.0000\n",
      "Epoch 198: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 82ms/step - loss: 3.5019e-06 - acc: 1.0000 - val_loss: 3.8483e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.4539e-06 - acc: 1.0000\n",
      "Epoch 199: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 83ms/step - loss: 3.4539e-06 - acc: 1.0000 - val_loss: 3.7894e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/200\n",
      "44/44 [==============================] - ETA: 0s - loss: 3.4049e-06 - acc: 1.0000\n",
      "Epoch 200: val_acc did not improve from 1.00000\n",
      "44/44 [==============================] - 4s 85ms/step - loss: 3.4049e-06 - acc: 1.0000 - val_loss: 3.7367e-06 - val_acc: 1.0000 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[105,   0],\n",
       "        [  0,  51]],\n",
       "\n",
       "       [[100,   0],\n",
       "        [  0,  56]],\n",
       "\n",
       "       [[107,   0],\n",
       "        [  0,  49]]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
